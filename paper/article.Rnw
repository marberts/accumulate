\documentclass[article,table]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------
%% recommended packages
\usepackage{orcidlink,thumbpdf,lmodern}

\usepackage{amsmath, amssymb}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage{tikz}


%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

% This allows referencing institutional authors. 
% See: https://tex.stackexchange.com/questions/162659/formatting-of-institution-as-author-with-natbib-and-numbered-references
\newcommand{\xfnm}[1][]{\ifx!#1!\else\unskip,\space#1\fi}



%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)
@


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation (and optionally ORCID link)
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Mark P.J. van der Loo~\orcidlink{0000-0002-9807-4686}\\
        Statistics Netherlands and University of Leiden}

\Plainauthor{Mark P.J. van der Loo}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{Split-Apply-Combine with Dynamic Grouping}
\Plaintitle{Split-Apply-Combine with Dynamic Grouping}
\Shorttitle{Split-Apply-Combine with Dynamic Grouping}


\Abstract{
foo
}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{data analysis, \proglang{R}}
\Plainkeywords{data analysis, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Mark P.J. van der Loo~\orcidlink{0000-0002-9807-4686}\\
  Research and Development\\
  Statistics Netherlands\\
  Henri Faasdreef 312\\
  2492JP Den Haag, the Netherlands\\
  E-mail: \email{mpj.vanderloo@cbs.nl}\\
  URL: \url{https://www.markvanderloo.eu}\\
  \emph{and}\\
  Leiden Institute of Advanced Computer Science (LIACS)\\
  University of Leiden\\
  P.O. Box 9512\\
  2300 RA Leiden, The Netherlands\\
}

\begin{document}


%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, and \code{} markup throughout the manuscript.



\section{Introduction}
The operation of splitting a data set into non-overlapping groups, computing an
aggregate for each group, and combining the results into a new dataset is one
of the most common operations in data analyses. Indeed, any software for data
analyses includes some functionality for this. For example, the combination of
\code{split}/\code{lapply}/\code{unsplit} as well as \code{aggregate} have been
a part of the \proglang{S} \citep{becker1988new} and \proglang{R} \citep{rcore}
languages for a long time. For \proglang{R} there are several packages that
implement functionality for this, including \pkg{plyr}
\citep{wickham2011split}, it's successor \pkg{dplyr} \citep{wickham2022dplyr},
it's drop-in replacement \pkg{poorman} \citep{eastwood2022poorman}, and
\proglang{R} packages \pkg{collapse} \citep{krantz2022collapse} and
\pkg{data.table} \citep{dowle2022datatable}. In \proglang{Python} the
\code{pandas} package implements several methods for grouping records and
aggregating over one or more columns in data frame objects. Similarly, the
\pkg{DataFrames} package for \proglang{julia} implements such functionality
\citep{kaminski2022dataframes}.

In all packages mentioned, the calculation for each group uses data available
within the group. However, there are valid use cases where a group aggregate is
determined using attributes from out-of-group entities. One example where this
occurs is in the area of small area estimation (see e.g.
\citet{molina2015sae}. Here, one wishes to estimate an aggregate for a group,
for example a geographical region, or a detailed population subset, where the
number of (sampled) observations so small that the variance of the estimate
would be unacceptably large. In small area estimation (SAE) one trades bias for
variance by `borrowing statistical strength' from out-of-group records. The
out-of-group records can be obtained, for example by combining the original
small group with a group of records that are deemed similar in certain
respects. A second area where out-of-group records play a role is in certain
hot-deck imputation methods \citep{andridge2010review}.  In
$k$-nearest-neighbours imputation for example, one finds a set of $k$ donor
records that are preferably in the same group, but this condition may be
relaxed if there are not enough records in the group.  In the \pkg{VIM} package
for \proglang{R} \citep{kowarik2016imputation}, this is controlled by a
combination of the Gower distance and setting conditions on minimal number of
donors. In practice, imputation is often performed via a fall-though scenario,
where one first tries to estimate a model within a group, but if the group is
too small for a reliable estimate of model parameters, the group is enlarged by
combining similar groups, similar to the small-area estimation scenario. A
third example comes from the field of statistical disclosure control (SDC)
\citep{hundepool2012statistical}. In this field, the so-called $p$\% rule
states that the value in a grouped aggregation may not be determined for more
than $p$ percent by a single entity. This occurs for example, when a certain
economic activity in a geographic region is dominated by a single company. In
such cases the value can either not be published (suppression), or one
publishes an aggregate over larger groups. Similarly for microdata there is the
concept of $k$-anonymity, which states that each combination of properties must
occur at least $k$ times in a data set so as to prevent reidentification or
disclosure of individual properties. One way to achieve this is to combine
detailed groupings based on categorical variables into larger groups. For
example by combining groups that in a hierarchical classification have the same
parent.

Both SAE and SDC are supported by \proglang{R} and other free software. Small
area estimation methodology has for example been implemented in the \pkg{sae}
package of \cite{molina2015sae} and the \pkg{hbsae} package of
\cite{boonstra2022hbsae}.  Disclosure control methodology is implemented in the
free and open source standalone software $\tau-$Argus for tabular data and
$\mu$-Argus for microdata \citep{argus2022}, and is also supported by
\proglang{R} packages \pkg{sdcMicro} and \pkg{sdcTable}
\citep{templ2015statistical, meindl2022sdctable}. Regarding imputation
methodology, the CRAN task view on missing
data\footnote{https://cran.r-project.org/web/views/MissingData.html} currently
lists 203 \pkg{R} packages that support some form of estimating missing data.
The \pkg{simputation} package \citep{loo2022simputation} seems to be the only
one that allows for some kind of fall-through scenario for selecting methods,
but it does not allow for dynamic grouping.

Summarizing, we see that on one hand there are many implementations available
for generic aggregation based on fixed groups. On the other hand there are
domain-specific implementation for methods where dynamic grouping of a set of
records plays a role. This paper presents a generic solution to
split-apply-combine aggregation where groups can be collapsed dynamically in
the form of an \pkg{R} package \pkg{accumulate} \cite{loo2022accumulate}.

The \code{accumulate} package serves the use case where a user wishes to
compute aggregates for a certain grouping of records. However, if a certain
instance of a group does not meet user-defined quality requirements, the set of
records is expanded by (recursively) collapsing the grouping according to a
user-defined scheme. For example, given some financial data on companies, one
wishes to compute the average profit to turnover ratio for each combination of
economic activity and size class. If for a certain combination of economic
activity and size class there are too few records for a reliable estimate, one
could drop size class and compute the average ratio over all records within a
certain economic activity. Or, one could choose to course-grain economic
activities by collapsing groups with activities that are deemed similar enough.


The package has been developed with the following design choices in mind.
First, the interface should be easy to learn for \proglang{R} users, and thus
should resemble existing popular interfaces where possible. Second, users
should be free to define any possibly multi-step, collapsing scheme.  Here, we
keep in mind that collapsing schemes may be constructed manually based on
domain knowledge and that users may want to experiment with several schemes
before deciding on a final solution. This calls for a certain separation of
concerns between defining collapsing schemes and applying them to data.  The
package should also support collapsing schemes that follow naturally from
hierarchical classification systems. Third, users should have the flexibility
to define any quality requirement on the grouping while common quality
requiremtns are supported out of the box. Common quality requirements include a
minimum number of records, or a minum fraction of records without missing
values, or a minimum number of records with non-zero values. 

The rest of this paper is organized as follows. In the next section we formally
define the concepts of the quality requirements and a collapsing scheme. We
describe the fundamental algorithm in pseudocode and compare it with the
standard grouping scenario. We also discuss the time and space complexity and
some caveats with implementation and performance. This section is mainly of
interest for developers who wish to include such functionality in their own
package. In Section~\ref{sect:accumulate} we describe \pkg{accumulate} package
and demonstrate how the design choices have determined its implementation.  We
give some toy examples to extensively discuss the input and output of the main
functions, discuss how to define tests in some detail, and also explain how one
can program over the functions in the package.  Section~\ref{sect:example}
works through a realistic example based on a synthetic dataset of economic
microdata. We conclude with a summary and outlook in
Section~\ref{sect:conclusion}.


\section{Dynamic grouping} \label{sect:dynamicgrouping}
In this section the illustrate the concept of dynamic grouping with a minimal
worked example. This example is not very realistic, but it is constructed
to be simple enough so the whole procedure can be followed in detail. 

We consider a data set with three categorical variables $A$, $B$ and $B_1$, and
one numerical variable $Y$.  Variable $A$ has levels $\{1, 2,3\}$ and variable
$B$ is a hierarchical classification with levels $\{11,12,13,21,22\}$. Variable
$B_1$ is a coursegraining of $B$: for each record the value for $B_1$ is the
first digit of $B$. Hence, $B_1$ has levels $\{1,2\}$. 

Our goal is to compute the sum $t_Y$ over $Y$, grouped by $A\times B$.  We
impose the condition that there must be at least three records in each group.
If a certain group $(a\in A,b\in B)$ has less than three records, we attempt
to compute the value for that group over records in $(a,b_1)$ where $b_1$ is
obtained by taking the first digit of $b$. If we then still have less than
three records, we take records of group $a$ to determine the value for $(a,b)$.

The following tables illustrate the idea. The left table represents the data
set to be aggregated by $A$ and $B$.  The table on the right represents the
output. Colors indicate which data was used. 
%
\begin{center}
\begin{tabular}{cccl}
\multicolumn{4}{l}{Input data}\\
\hline
$A$ & $B$ & $B1$ & $Y$\\
\hline
\cellcolor{red!25}   1 &\cellcolor{red!25}   11 &                1       & \cellcolor{red!25}   1   \\
\cellcolor{red!25}   1 &\cellcolor{red!25}   11 &                1       & \cellcolor{red!25}   2   \\
\cellcolor{red!25}   1 &\cellcolor{red!25}   11 &                1       & \cellcolor{red!25}   4   \\
\cellcolor{green!25} 2 &                     12 & \cellcolor{green!25} 1 & \cellcolor{green!25} 8   \\
\cellcolor{green!25} 2 &                     12 & \cellcolor{green!25} 1 & \cellcolor{green!25} 16  \\
\cellcolor{green!25} 2 &                     13 & \cellcolor{green!25} 1 & \cellcolor{green!25} 32  \\
\cellcolor{blue!25}  3 &                     21 &                2       & \cellcolor{blue!25}  64  \\
\cellcolor{blue!25}  3 &                     22 &                2       & \cellcolor{blue!25}  128 \\
\cellcolor{blue!25}  3 &                     12 &                1       & \cellcolor{blue!25}  256 \\
\hline
\end{tabular}\hspace{1cm}\begin{tabular}{ccl}
\multicolumn{3}{l}{Output}\\
\hline
$A\times B$ & \code{level} & $t_Y$\\
\hline
\rowcolor{red!25}   1 11        & 0             & 7  \\
\rowcolor{green!25} 2 12        & 1             & 56 \\
\rowcolor{green!25} 2 13        & 1             & 56 \\
\rowcolor{blue!25}  3 21        & 2             & 448\\
\rowcolor{blue!25}  3 22        & 2             & 448\\
\rowcolor{blue!25}  3 12        & 2             & 448\\
\hline
\end{tabular}
\label{eq:example}
\end{center}
%
The First row in the output represents group $(A=1,B=11)$. The collapsing level
is zero, which means that no collapsing was necessary. Indeed, in the data
table we see that there are three rows with $A=1$ and $B==11$ with $Y$ values
$1, 2$, and $7$, resulting in $t_Y=7$ for this group.

Next, we try to compute the total for group $(A=2,B=12)$ (in green) but find
that there are only two such rows. We now define a new group $(A=2,B_1=1)$ and
find that there are three records in that group so we get 
$t_Y=8+16+32=56$.  Similarly, there is only one record with $(A=2,B=13)$. 
Collapsing groups to $(A=2,B1=1)$, yieds again $t_y=56$.

Finally, for $(A=2,B=21)$ there is only a single record. Collapsing to
$(A=2,B1=2)$ yields only two records, so we need to collapse further to $(A=2)$
and finally obtain three records.  This yields $t_Y=64+128+256=448$. Similarly,
the groups $(A=3, B=22)$ and $(A=3,B=12)$ are collapsed to $(A=2)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5%%%%%%%
\section{The accumulate R package}\label{sect:accumulate}
Grouped aggregation with a fall-though scenario based on a collapsing scheme
requires a fair amount of specification by the user. Besides the data to be
aggregated, one needs to specify the method(s) of aggregation, the collapsing
scheme, and the condition to decide whether a subset is fit for aggregation or
a next collapse is necessary. There are two main functions in \pkg{accumulate}
that offer slightly different interfaces.
\begin{verbatim}
  accumulate(data, collapse, test, fun, ...)
  cumulate(data, collapse, test, ...)
\end{verbatim}
Here \texttt{data} is a data frame holding data to be aggregated;
\texttt{collapse} represents the collapse sequence (as a \class{formula} or a
\class{data frame}), and \texttt{test} is a function that accepts a subset of
\texttt{data} and returns a boolean that indicates whether a subset is suited
for aggregation or not. In \code{accumulate()}, the parameter \code{fun}
represents an aggregation function that is applied to every column of
\code{data}, and the ellepsis (\code{...}) is for arguments that are passed as
extra argument to \code{fun}. The interface of \code{accumulate()} is modeled
after the \code{aggregate()} function in \proglang{R}.  In \code{cumulate}, the
ellepsis is a sequence of comma-separated \code{name=expression} pairs in the
style of \code{summarise()} from the \code{dplyr} package.

The output of both functions are of the same form. The columns of the output
data frame and can schematically be represented as follows. 
\begin{verbatim}
  [Grouping Variables, Collapse level, Output aggregates]
\end{verbatim}
The first columns represent the variables that define the output grouping, the
next column is an integer that indicates the level of collapsing used to
compute the aggregate (0 indicating no collapse), and the last set of columns
store the aggregates.

Both functions support two different interfaces for specifying collapsing
schemes through the \code{collapse} parameter. The first and most general is
the formula interface, which requires that the collapsing sequence is
represented as variables in the data set to be aggregated. The second is the
tabular interface, where a table similar to~\eqref{eq:tablerep} is expected. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The formula interface}\label{sect:formula}
We will use the example of Section~\ref{sect:dynamicgrouping} to illustrate
the formula interface.
<<>>=
library(accumulate)
input <- data.frame(
     A  = c(1,1,1,2,2,2,3,3,3)
   , B  = c(11,11,11,12,12,13,21,22,12)
   , B1 = c(1,1,1,1,1,1,2,2,1)
   , Y  = 2^(0:8)
)

cumulate(input, collapse = A*B ~ A*B1 + A
        , test = function(d) nrow(d) >= 3, tY = sum(Y) )
@
%
Consider the formula \code{A*B ~ A*B1 + A} in the call to \code{cumulate()}.
The left-hand-side \code{A*B} is the target output grouping. The
ritght-hand-side is to be interpreted as the collapsing sequnce: if an instance
of \code{A*B} does not pass the test, then collapse to \code{A*B1}, and if that
does not pass the test collapse to \code{A}. If this final grouping also does
not pass the test, the result is \code{NA}.

Summarizing, the formula interface is always of the follwing form.
\begin{verbatim}
  Target grouping ~ Alternative1 + Alternative2 + ... + AlternativeN
\end{verbatim}



It is possible to get the same result with \code{accumulate()}. This will cause
summation over all variables that are not used in the formula object. In the
below example we also introduce the helper function \code{min_records()}.
<<>>=
input$Y2 <- 3^(0:8)
accumulate(input, collapse = A*B ~ A*B1 + A
          , test = min_records(3), fun = sum)
@
This means that for experimentation, users must be careful to exclude categorical
variables that are not used from the input data set. For example, if \code{B1}
is not used, we get the following.
<<>>=
accumulate(input[-3], collapse = A*B ~ A
          , test = min_records(3), fun = sum)
@

The formula interface allows users to quickly experiment with different
collapsing schemes. It does require that all categorical variables have been
added to the data. As an alternative, one can use the data frame specification,
which allows for a further separation between defining the collapsing scheme
and the actual data processing.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The data frame interface} \label{sect:dfint}
The data frame interface is somewhat limited because it only allows for a
single grouping variable. The advantage however setting up a collapsing scheme
in the form of a table closely connects to domain knowledge and allows
fine-grained control on how groups are collapsed.

In order to use the data frame interface, the input dataset must include the
most fine-grained grouping variable. In the running example this is $A\times
B$, so we need to combine that into a single variable, and remove the other
ones.
<<>>=
input1 <- input
input1$AB <- paste(input$A, input$B, sep="-")
input1 <- input1[-(1:3)]
input1
@
We now define the collapsing scheme as follows.
<<>>=
csh <- data.frame(
  AB  = c("1-11", "2-12","2-13","3-21","3-22","3-12")
, AB1 = c("1-1" , "2-1" ,"2-1" ,"3-2" ,"3-2" ,"3-1" )
, A   = c("1"   , "2"   ,"2"   ,"3"   ,"3"   ,"3"   ))
csh
@
In this data frame, two consecutive columns should be read as a child-parent
relation. For example, in the first collapsing step the groups defined by
\code{AB == "2-12"} and \code{AB=="2-13"} both collapse to \code{AB1 == "2-1"}.
In this artificial example the codes do not mean anything, but in realistic
cases where codes represent a (hierarchical) classification, domain experts
usually have a good grasp of which codes can be combined. 

The calls to \code{cumulate()} and \code{accumulate()} now look as follows.
<<>>=
accumulate(input1, collapse=csh, test = min_records(3), sum)
cumulate(input1, collapse=csh, test=min_records(3), tY=sum(Y), tY2 = sum(Y2))
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Balanced and unbalanced hierarchical classifications}
Hierarchical classifications are abundant in (official) statistics.
They represent a classification of entities into non-overlapping, nested
groupings. Examples include the International Standard Industrial Classification
of Economic Activities (ISIC, \citet{un2022isic}), the related 
Statistical Classification of Economic Activities in Europe (NACE, \citet{eu2006nace})
and the Euroean Skills, Competences, Qualifications and Occupations classification
(ESCO, \citet{eu2022esco}).

Hierarchical classifications offer a natural mechanism for collapsing fine-graind groupings
into larger groups because of the parent-child relationships. As an example consider
a small piece of the NACE classification.

\begin{center}
\begin{tikzpicture}
\tikzstyle{level 1}=[level distance=10mm,sibling distance=40mm]
\tikzstyle{level 2}=[level distance=10mm,sibling distance=10mm]
  \node {\code{01}}
    child { node {\code{011}}
      child {node {\code{0111}}}
      child {node {\code{0112}}} 
      child {node {\code{0113}}} }
    child { node {\code{012}}
      child {node {\code{0121}}}
      child {node {\code{0122}}} 
      child {node {\code{0123}}} 
      child {node {\code{0124}}} };
\end{tikzpicture}
\end{center}
Here, the hierarchy suggest to collapsing $\{0111,0112,0113\}$ into $\{011\}$
(when needed) and similar for the right branch. The second level of collapsing
would combine $\{012\}$ with $\{011\}$ into $\{01\}$. The \pkg{accumulate}
package comes with a helper function that creates the collasping scheme from the
lowest-level digits.
<<>>=
nace <- c("0111","0112","0113","0121","0121","0122","0123","0124")
csh_from_digits(nace, levels=2)
@ 
Here, the parameter \code{levels} determines how many collapsing steps will be
computed. Since all codes are prepended with zero, there is no need to collapse
01 any further. The output can be used as argument to the \code{collapse}
parameter of the \code{accumulate()} or \code{cumulate} functions.

The situation becomes a little more involved when hierarchical classifcations
form a tree such that the distance from leave to trunk is not the same for all
leaves (unbalanced tree). This occurs in practice, for example when local
organisations create an extra level of detail for some, but not all leaves.
Below is an example of such a situation.
%
\begin{center}
\begin{tikzpicture}
\tikzstyle{level 1}=[level distance=10mm,sibling distance=50mm]
\tikzstyle{level 2}=[level distance=10mm,sibling distance=10mm]
\tikzstyle{level 2}=[level distance=10mm,sibling distance=15mm]
  \node {\code{01}}
    child { node {\code{011}}
      child {node {\code{0111}}}
      child {node {\code{0112}}} 
      child {node {\code{0113}}} }
    child { node {\code{012}}
      child {node {\code{0121}}}
      child {node {\code{0122}}} 
      child {node {\code{0123}}} 
      child {node {\code{0124}}
       child {node {\code{01241}}}
       child {node {\code{01242}}} } };
\end{tikzpicture}
\end{center}
%
In this case, not all leaves can be collapsed with the same number of
collapsing levels. This provides an issue for specifying the collapsing
sequence as it now depends on the leaf where you start how many collapsing
levels are possible. It also complicates interpretability of the result as the
collapsing level reported in the output, now means different things for
different target groups. The solution chosen in \pkg{accumulate} is to extend
the tree by making copies of the leaves that are not on the lowest level as
follows.
%
\begin{center}
\begin{tikzpicture}
\tikzstyle{level 1}=[level distance=10mm,sibling distance=70mm]
\tikzstyle{level 2}=[level distance=10mm,sibling distance=20mm]
\tikzstyle{level 3}=[level distance=10mm,sibling distance=15mm]
  \node {\code{01}}
    child { node {\code{011}}
      child {node {\code{0111}}
       child {node{\code{0111}}}} 
      child {node {\code{0112}} 
       child {node{\code{0112}}}} 
      child {node {\code{0113}}
       child {node{\code{0113}}}} } 
    child { node {\code{012}}
      child {node {\code{0121}}
       child {node {\code{0121}}}}
      child {node {\code{0122}}
       child {node{\code{0122}}}} 
      child {node {\code{0123}}
       child {node{\code{0123}}}} 
      child {node {\code{0124}}
       child {node {\code{01241}}}
       child {node {\code{01242}}} } };
\end{tikzpicture}
\end{center}
%
The tradeoff is that although there may be some extra calculations in the case
where a leaf is collapsed to itself. The gain is that the specification of the
calculation as well as the interpretation of the results are now uniform
accross all hierarchical classifications. Again, using \code{csh_from_digits()}
deriving the collapsing scheme can be automated.
<<>>=
nace <- c("0111","0112","0113","0121","0122","0123","01241","01242")
csh_from_digits(nace,levels=3)
@







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Specifying tests}
The \code{test} parameter of \code{cumulate()} and \code{accumulate} accepts a
function that takes a subset of the data and must returns a boolean. For common
test conditions, including requiring a minimal number of records, or a minimal
number or fraction of complete records there are helper functions available.
%
\begin{center}
\begin{tabular}{ll}
\code{min_records(n)}        & At least $n$ records.\\ 
\code{min_complete(n,vars)}  & At least $n$ records complete records for variables \code{vars}.\\
\code{frac_complete(r,vars)} & At least $100r$\% complete records for variables \code{vars}.\\
\end{tabular}
\end{center}

Second, for multiple, possibly complex requirements on variables users can
express conditions with the \pkg{validate} package \cite{loo2021data}.  The
\pkg{validate} packages offers a domain-specific language for expressing,
manipulating, and investigating conditions on datasets. It's core concept is a
list of `data validation rules' stored as a \class{validator} object. A
\class{validator} object is constructed with the enopymous function
\code{validator()}.  For example, to demand that there are at least 3 rows in a
group, and that there are at least three records where $Y\geq 2$ we create the
following ruleset.
<<>>=
library(validate)
rules <- validator(nrow(.) >= 3, sum(Y >= 2) >= 3)
rules
@
Here the \code{.} refers to the dataset as a whole, while rules that can be
evaluated within the dataset can be written as boolean \proglang{R}
expressions.

We will apply these conditions to the \code{input} dataset that was 
constructed in Section~\ref{sect:formula}. As a reminder we print
the first few records.
<<>>=
head(input, 4)
@
We use \code{A*B ~ A*B1 + B1} as collapsing scheme. The function
\code{from_validator} passes the requirements as a test function to
\code{accumulate()} (or \code{cumulate()}.
<<>>=
accumulate(input, collapse = A*B ~ A*B1 + B1,
    test = from_validator(rules), fun=sum)
@
Note that for target groups $(A=2,B=21)$ and $(A=2,B=22)$ none of the available
collapsing levels lead to a group that satisfied all conditions. Therefore the
collapsing level and output variables are all missing (\code{NA}).

The third and most flexible way for users to express tests is to write a custom
testing function. The requirements are that it must work on any subset of a
data frame, including a dataset with zero rows. The previous example can
thus also be expressed as follows.
<<>>=
my_test <- function(d) nrow(d) >= 3 && sum(d$Y >= 2) >=3
accumulate(input, collapse = A*B ~ A*B1 + B1, test=my_test, fun=sum)
@

It is easy to overlook some edge cases when specifying test functions.
Recall that a test function is required to return \code{TRUE} or \code{FALSE},
regardless of the data circumstances. The only thing that a test function
can assume is that the received data set is a subset of records from the
dataset to be aggregated. As a service to the user, \pkg{accumulate} exports
a function that checks a test function against common edge cases, including
the occurrence of missing values, a dataset with zero rows, and the full
dataset. The smoke test checks whether the output is \code{TRUE} or \code{FALSE}
under all circumstances and also reports erros, warnings, and messages.
It accepts a (realistic) dataset and a testing function and prints
test results to the console.
<<>>=
smoke_test(input, my_test)
@
By default only failing tests are printed. In this case our test function
is not robust against missing values for $Y$. This can be remedied
by adding \code{na.rm=TRUE} to the \code{sum()} in the test function.
<<>>=
my_test1 <- function(d) nrow(d) >= 3 && sum(d$Y>=2, na.rm=TRUE) >=3 
smoke_test(input, my_test1)
@
The smoke test is aimed at prevening complicated stack traces when errors occur
in a call to \code{accumulate()} or \code{cumulate()}.  Users should be aware
that it does not guarantee correctness of the results, only robustness against
certain edge cases.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extensive example: economic data} \label{sect:example}


\section{Formal descriptions and algorithms}
In this section we give a formal description of the algorithms. In order to be
unambiguous and to represent the algorithms compactly, we introduce some
notation. The notation also makes the pseudocode independent of the
implementation language used, or how the data is represented. 

We start by giving the algorithm for ordinary split-apply-combine, so it is
easy to see where the algorithm must be generalized to allow for a collapsing
scheme.


\subsection{Split-apply-combine}
In order to analyse a data set group by group we need to specify a data set, a
way to split it into groups, and a function that takes a subset of data and
returns an aggregate. Let us introduce some notation for that.

Let $U$ be a finite set, and let $\phi$ be a function that accepts a subset of
$U$ and returns a value in some domain $X$. Here, $U$ represents a data set and
$\phi$ an aggregating function. We split $U$ into groups using the following
notation. Let $A$ be finite set that has no more elements than $U$, and let $f$
be a function that takes an element of $U$ and returns a value in $A$. We can
think of $A$ as a set of group labels, and $f$ as the function that assigns a
label to each element of $U$. This way, $f$ divides $U$ into non-overlapping
groups, since every element in $U$ can only get one label.  In formal
terminology we say that $f:U\to A$ is a \emph{partition} of $U$.

In this notation, any split-apply-combine operation can be computed with the
following algorithm.

\begin{algorithm}[H]
\caption{Split-Apply-Combine}
\label{alg:sac}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{A finite set $U$, an aggregator $\phi: 2^U\to X$, and a partition $f:U\to A$}
\Output{$R$: the value of $\phi$ for every part of $U$ as a set of pairs $(a,x)\in
A\times X$ }

$R = \{\}$\;
\For{$a\in A$}{
  $d=f^{-1}(\{a\})$\tcp*{get subset of $U$}
  $R = R\cup \{(a,\phi(d))\}$\tcp*{aggregate and add to result}
}
\end{algorithm}

Here, the output is collected in a set $R$ (this set is technically a function
$A\to X$ since it pairs for each $a\in A$ with one element of $X$). We also
introduced some new notation: the notation $2^U$ stands for `the set of all
subsets of $U$'. The notation $f^{-1}$ stands for the formal \emph{inverse} of
$f$. That is: you give it a set of labels from $A$ and it returns all the
elements of $U$ that have any of those labels. This definition is slightly too
general for the current algorithm, but it will come in handy when we move to
collapsing groups.

Let us see how the elements $U$, $f:U\to A$, and $\phi$ are implemented
in practice. Consider the signature of the R \code{aggregate()} function for data
frames (we skip arguments that are not important for the discussion).
\begin{verbatim}
  aggregate(x, by, FUN)
\end{verbatim}
Here, \code{x} is a data frame where each column is a variable $U$ to be
aggregated.  The parameter \code{by} represents the record-wise label(s) that
are used for grouping. So the function $f\to A$ is implemented by forcing the
user to make sure that the position of each label in \code{by} corresponds to
the row number in the data frame \code{x}. The argument \code{FUN} is the
function $\phi$ that aggregates each subset of \code{x}.  Here is an example of how
a user might call this function from the R prompt.
%
<<>>=
aggregate(iris[1:2], by=iris["Species"], FUN=mean)
@
%
Note that the correspondence in position of the \texttt{Species} label and the
record position is implemented by taking them from the same data frame. The output
also reveals in the first column the set $A$: each row corresponds to a unique
value in \texttt{Species} column.


\subsection{Split-apply-combine with collapsing groups}
\label{sect:saccg}
The goal of the algorithm is to compute a value for each part of a dataset,
possibly using values from a larger subset. The input of the algorithm consists
again of a finite set $U$ and an aggregation function $\phi$ that takes a subset
of $U$ and returns a value in some domain $X$.  Compared to
Algorithm~\ref{alg:sac} we need two extra inputs. First, we need a user-defined
function that checks whether a given subset $d$ of $U$ is suitable for
computing $\phi(d)$.  We will call this function $\beta: 2^U\to \mathbb{B}$,
where $\mathbb{B}=\{\texttt{True},\texttt{False}\}$.  Typical tests are
checking whether there are sufficient records available, or whether certain
variables have a low enough fraction of missing values. Second, we need a
\emph{collapsing scheme}
\begin{equation}
U\xrightarrow{f_0}A_0\xrightarrow{f_1}A_1\xrightarrow{f_2}\cdots\xrightarrow{f_n}A_n.
\label{eq:collapsingsequence}
\end{equation}
A collapsing scheme is a sequence of partitions where each $f_i:A_{i-1}\to A_i$
partitions $A_{i-1}$ into $|A_i|$ groups. 

Collapsing schemes can be represented as tables. For example, the European NACE
classification is a hierarchical classification of economic activity. Each
group is encoded as a 4-digit code where the first digit indicates the highest
grouping level, and the last digit the most detailed level. This induces a
collapsing sequence by combining groups that have the same parent in the
hierarchy. Below is an example, based on a small piece of the NACE code list.
%
\begin{equation}
\begin{tabular}{rrr}
\hline
 $A_0$& $A_1$ & $A_2$\\
\hline
0111&   011 & 01\\
0112&   011 & 01\\
0113&   011 & 01\\
0121&   012 & 01\\
0122&   012 & 01\\
0123&   012 & 01\\
0124&   012 & 01\\
\hline
\end{tabular}
\label{eq:tablerep}
\end{equation}

Here, $A_0$ is the most detailed partition, splitting a dataset into seven
groups, relating to $\{0111,\ldots,0124\}$. In $A_1$ there are two groups.
The first, labeled $01.1$ is constructed by combining $0111, 0112$ and
$0113$ and the second, labeled $01.2$ is constructed by combining
$0121,\ldots 01.24$. Finally, in $A_2$ all groups are combined.

To compute an aggregate for records labeled with $0111$, the algorithm will
select those records and test whether the subset is suitable. If not, it will
move to $f_1(0111)=011$ and find all the groups in $A_0$ that map to 011 as
follows.
\begin{displaymath}
f_1^{-1}(\{011\}) = \{0111, 0112, 0113\}.
\end{displaymath}
It will then move on and find all records labeled with any of those three
group labels, and test again. If this new data set is suitable, the result is
computed, if not, another collapse takes place. The whole procedure is repeated 
for each element in $A_0$.




In order to generalize this algorithm towards generic collapsing schemes, we
introduce the function $F_k:A_0\to A_k$, which accepts a label in $A_0$ and
returns the corresponding label in $A_k$. This can formally be defined as
consecutive application of $f_1, f_2,\cdots, f_k$ to an element of $A_0$. In
notation
\begin{equation*}
F_k \equiv f_k\circ f_{k-1}\circ\cdots\circ f_1.
\end{equation*}
Similarly we define $F^{-1}_k:2^{A_k}\to 2^{A_0}$ as the composition
\begin{equation*}
F_k^{-1} \equiv f_1^{-1}\circ f_2^{-1}\circ\cdots\circ f_k^{-1}.
\end{equation*}
This function asks for a set of labels in $A_k$ and returns all the labels in
$A_0$ that are mapped to those labels via the collapsing sequence of
Equation~\eqref{eq:collapsingsequence}.  We are now ready to formally write
down our main Algorithm~\ref{alg:saccg}.

%
\begin{algorithm}[H]
\caption{Split-Apply-Combine with Collapsing Groups}
\label{alg:saccg}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{A finite set $U$, an aggregator $\phi: 2^U\to X$, a test function $\beta: 2^U\to \mathbb{B}$,
      and a collapsing sequence $U\xrightarrow{f_0}A_0\cdots \xrightarrow{f_n}  A_n$}

\Output{$R$: the value of $\phi$ for every part of $U$, for which a suitable
collapsing group can be found, as a set of triples $(a,k,x)\in A_0\times
\underline{n}\times X $ where $\underline{n}=\{0,1,\ldots,n\}$.}

$R = \{\}$\;
\For{$a\in A_0$}{
  $i=0$\;
  $d = f_0^{-1}(\{a\})$\tcp*{get subset of $U$}
  \While{$i<n \land \lnot\beta(d)$}{
    $i = i+1$ \tcp*{Increase collapse level} 
    $d = (f_0^{-1}\circ F_i^{-1}\circ F_i)(a)$ \tcp*{Collapse and get subset}
  }
  \If{$i<n \lor \beta(d)$}{ \label{line:cond}
    $R = R\cup \{(a, i,\phi(d))\}$\;
  } 
}

\end{algorithm}
%
Since the used level of collapsing is determined dynamically by data
circumstances in $U$, the algorithm also reports the collapsing level used to
compute a value foe each member of $A_0$. For each member of $A_0$ there is a
triple $(a,i,\phi(d))$, where $a\in A_0$ is the label to which the value
$\phi(d)$ pertains, and $i$ is the number of collapses applied to reach a
suitable dataset. The condition in Line~\ref{line:cond} ensures that if no
suitable dataset is found after the whole collapsing sequence has been
executed, then no answer is returned.

\subsection{Performance, memory, paralellization}
Both Algorithms~\ref{alg:sac} and \ref{alg:saccg} need only $\mathcal{O}(|U|)$
memory. The time complexity of Algorithm~\ref{alg:sac} is $\mathcal{O}(|A||U|)$
($|A|$ the number of groups occurring in the data, and $|U|$ the number of
records) but the actual running time is also determined by the form of the
aggregating function $\phi$.  For simple aggregating functions such as
summation or counting, the output can be computed with a single pass through
$|U|$. \proglang{R} packages such as \pkg{collapse} and \pkg{data.table} have
implemented specialized low level (\proglang{C}/\proglang{C++}) code for fast
groupwise aggregation for some commonly occurring aggregating functions.
Generic paralellization of Algorithm~\ref{alg:sac} can be done by paralellising
over the grouping values $a\in A$: each worker node gets access to a set of
records corresponding to a single group and computes the local aggregate.

Algorithm~\ref{alg:saccg} has time complexity of $\mathcal{O}((n+1)|A_0||U|)$,
Namely, in the worst case, for each $a\in A_0$ we need to collapse to the
maximum level $A_n$. This means that for each $a\in A_0$ we need to collapse
$n$ times, plus the initial attempt of computing aggregate without collapsing.
The fact that we cannot generically exclude records from a calculation means
that a generic paralellisation scheme would need to ensure that each worker
node has access to the full dataset $|U|$. There are two types of options to do
this: either mutliple workers gain read-only access to $U$ or each worker gets
a copy of $U$. In the first case the workers may suffer from colliding
read-operations which hampers linear performance improvement as a function of
the number of workers. In the second case, one needs as many copies of the data
as there are threads. It is possible to compute beforehand whether the dataset
can be split into subsets that can be treated independently by determining all
the collapsed subsets. However, this is almost as expensive as running the
algorithm completely since it only skips the calculation of aggregates after
collapsing. 


Summarizing, we see that split-apply-combine with collapsing groups has a
higher worst-case complexity than regular split-apply-combine. Moreover,
generic paralellization needs to either consider a specific use case (e.g.
large or small datasets) or require user input that guides the choice of
paralellisation scheme.






\section{Summary and conclusion}\label{sect:conclusion}

\begin{itemize}
\item{Record-wise results}
\item{Record-wise imputation}
\item{Representation of hierarchical classifications: parent-child lists vs tabular}
\end{itemize}

\newpage
\bibliography{refs}




\end{document}

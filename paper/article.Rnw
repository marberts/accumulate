\documentclass[article,table]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------
%% recommended packages
\usepackage{orcidlink,thumbpdf,lmodern}

\usepackage{amsmath, amssymb}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage{tikz}

% shortcut for surjection arrow, and surjection version of xrightarrow
\newcommand{\onto}{\twoheadrightarrow}
\newcommand\xonto[2][]{%
  \mathrel{\ooalign{$\xrightarrow[#1\mkern4mu]{#2\mkern4mu}$\cr%
  \hidewidth$\rightarrow\mkern4mu$}}
}



%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}

% This allows referencing institutional authors. 
% See: https://tex.stackexchange.com/questions/162659/formatting-of-institution-as-author-with-natbib-and-numbered-references
\newcommand{\xfnm}[1][]{\ifx!#1!\else\unskip,\space#1\fi}


%% For Sweave-based articles about R packages:
%% need no \usepackage{Sweave}
\SweaveOpts{engine=R, eps=FALSE, keep.source = TRUE}
<<preliminaries, echo=FALSE, results=hide>>=
options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)
@


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation (and optionally ORCID link)
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Mark P.J. van der Loo~\orcidlink{0000-0002-9807-4686}\\
        Statistics Netherlands and University of Leiden}

\Plainauthor{Mark P.J. van der Loo}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{Split-Apply-Combine with Dynamic Grouping}
\Plaintitle{Split-Apply-Combine with Dynamic Grouping}
\Shorttitle{Split-Apply-Combine with Dynamic Grouping}


\Abstract{
foo
}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{data analysis, \proglang{R}}
\Plainkeywords{data analysis, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Mark P.J. van der Loo~\orcidlink{0000-0002-9807-4686}\\
  Research and Development\\
  Statistics Netherlands\\
  Henri Faasdreef 312\\
  2492JP Den Haag, the Netherlands\\
  E-mail: \email{mpj.vanderloo@cbs.nl}\\
  URL: \url{https://www.markvanderloo.eu}\\
  \emph{and}\\
  Leiden Institute of Advanced Computer Science (LIACS)\\
  University of Leiden\\
  P.O. Box 9512\\
  2300 RA Leiden, The Netherlands\\
}

\begin{document}


%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, and \code{} markup throughout the manuscript.



\section{Introduction}
The operation of splitting a data set into non-overlapping groups, computing an
aggregate for each group, and combining the results into a new dataset is one
of the most common operations in data analyses. Indeed, any software for data
analyses includes some functionality for this. For example, the combination of
\code{split}/\code{lapply}/\code{unsplit} as well as \code{aggregate} have been
a part of the \proglang{S} \citep{becker1988new} and \proglang{R} \citep{rcore}
languages for a long time. For \proglang{R} there are several packages that
implement functionality for this, including \pkg{plyr}
\citep{wickham2011split}, it's successor \pkg{dplyr} \citep{wickham2022dplyr},
it's drop-in replacement \pkg{poorman} \citep{eastwood2022poorman}, and
\proglang{R} packages \pkg{collapse} \citep{krantz2022collapse} and
\pkg{data.table} \citep{dowle2022datatable}. In \proglang{Python} the
\code{pandas} package implements several methods for grouping records and
aggregating over one or more columns in data frame objects. Similarly, the
\pkg{DataFrames} package for \proglang{julia} implements such functionality
\citep{kaminski2022dataframes}.

In all packages mentioned, the calculation for each group uses data available
within the group. However, there are valid use cases where a group aggregate is
determined using attributes from out-of-group entities. One example where this
occurs is in the area of small area estimation (see e.g.
\citet{molina2015sae}. Here, one wishes to estimate an aggregate for a group,
for example a geographical region, or a detailed population subset, where the
number of (sampled) observations so small that the variance of the estimate
would be unacceptably large. In small area estimation (SAE) one trades bias for
variance by `borrowing statistical strength' from out-of-group records. The
out-of-group records can be obtained, for example by combining the original
small group with a group of records that are deemed similar in certain
respects. A second area where out-of-group records play a role is in certain
hot-deck imputation methods \citep{andridge2010review}.  In
$k$-nearest-neighbours imputation for example, one finds a set of $k$ donor
records that are preferably in the same group, but this condition may be
relaxed if there are not enough records in the group.  In the \pkg{VIM} package
for \proglang{R} \citep{kowarik2016imputation}, this is controlled by a
combination of the Gower distance and setting conditions on minimal number of
donors. In practice, imputation is often performed via a fall-though scenario,
where one first tries to estimate a model within a group, but if the group is
too small for a reliable estimate of model parameters, the group is enlarged by
combining similar groups, similar to the small-area estimation scenario. A
third example comes from the field of statistical disclosure control (SDC)
\citep{hundepool2012statistical}. In this field, the so-called $p$\% rule
states that the value in a grouped aggregation may not be determined for more
than $p$ percent by a single entity. This occurs for example, when a certain
economic activity in a geographic region is dominated by a single company. In
such cases the value can either not be published (suppression), or one
publishes an aggregate over larger groups. Similarly for microdata there is the
concept of $k$-anonymity, which states that each combination of properties must
occur at least $k$ times in a data set so as to prevent reidentification or
disclosure of individual properties. One way to achieve this is to combine
detailed groupings based on categorical variables into larger groups. For
example by combining groups that in a hierarchical classification have the same
parent.

Both SAE and SDC are supported by \proglang{R} and other free software. Small
area estimation methodology has for example been implemented in the \pkg{sae}
package of \cite{molina2015sae} and the \pkg{hbsae} package of
\cite{boonstra2022hbsae}.  Disclosure control methodology is implemented in the
free and open source standalone software $\tau-$Argus for tabular data and
$\mu$-Argus for microdata \citep{argus2022}, and is also supported by
\proglang{R} packages \pkg{sdcMicro} and \pkg{sdcTable}
\citep{templ2015statistical, meindl2022sdctable}. Regarding imputation
methodology, the CRAN task view on missing
data\footnote{https://cran.r-project.org/web/views/MissingData.html} currently
lists 203 \pkg{R} packages that support some form of estimating missing data.
The \pkg{simputation} package \citep{loo2022simputation} seems to be the only
one that allows for some kind of fall-through scenario for selecting methods,
but it does not allow for dynamic grouping.

Summarizing, we see that on one hand there are many implementations available
for generic aggregation based on fixed groups. On the other hand there are
domain-specific implementation for methods where dynamic grouping of a set of
records plays a role. This paper presents a generic solution to
split-apply-combine aggregation where groups can be collapsed dynamically in
the form of an \pkg{R} package \pkg{accumulate} \cite{loo2022accumulate}.

The \code{accumulate} package serves the use case where a user wishes to
compute aggregates for a certain grouping of records. However, if a certain
instance of a group does not meet user-defined quality requirements, the set of
records is expanded by (recursively) collapsing the grouping according to a
user-defined scheme. For example, given some financial data on companies, one
wishes to compute the average profit to turnover ratio for each combination of
economic activity and size class. If for a certain combination of economic
activity and size class there are too few records for a reliable estimate, one
could drop size class and compute the average ratio over all records within a
certain economic activity. Or, one could choose to course-grain economic
activities by collapsing groups with activities that are deemed similar enough.


The package has been developed with the following design choices in mind.
First, the interface should be easy to learn for \proglang{R} users, and thus
should resemble existing popular interfaces where possible. Second, users
should be free to define any possibly multi-step, collapsing scheme.  Here, we
keep in mind that collapsing schemes may be constructed manually based on
domain knowledge and that users may want to experiment with several schemes
before deciding on a final solution. This calls for a certain separation of
concerns between defining collapsing schemes and applying them to data.  The
package should also support collapsing schemes that follow naturally from
hierarchical classification systems. Third, users should have the flexibility
to define any quality requirement on the grouping while common quality
requirements are supported out of the box. Common quality requirements include a
minimum number of records, or a minimum fraction of records without missing
values, or a minimum number of records with non-zero values. 

The rest of this paper is organized as follows. In the next section we formally
define the concepts of the quality requirements and a collapsing scheme. We
describe the fundamental algorithm in pseudocode and compare it with the
standard grouping scenario. We also discuss the time and space complexity and
some caveats with implementation and performance. This section is mainly of
interest for developers who wish to include such functionality in their own
package. In Section~\ref{sect:accumulate} we describe \pkg{accumulate} package
and demonstrate how the design choices have determined its implementation.  We
give some toy examples to extensively discuss the input and output of the main
functions, discuss how to define tests in some detail, and also explain how one
can program over the functions in the package.  Section~\ref{sect:example}
works through a realistic example based on a synthetic dataset of economic
microdata. We conclude with a summary and outlook in
Section~\ref{sect:conclusion}.


\section{Dynamic grouping} \label{sect:dynamicgrouping}
In this section the illustrate the concept of dynamic grouping with a minimal
worked example. This example is not very realistic, but it is constructed
to be simple enough so the whole procedure can be followed in detail. 

We consider a data set with three categorical variables $A$, $B$ and $B_1$, and
one numerical variable $Y$.  Variable $A$ has levels $\{1, 2,3\}$ and variable
$B$ is a hierarchical classification with levels $\{11,12,13,21,22\}$. Variable
$B_1$ is a course graining of $B$: for each record the value for $B_1$ is the
first digit of $B$. Hence, $B_1$ has levels $\{1,2\}$. 

Our goal is to compute the sum $t_Y$ over $Y$, grouped by $A\times B$.  We
impose the condition that there must be at least three records in each group.
If a certain group $(a\in A,b\in B)$ has less than three records, we attempt
to compute the value for that group over records in $(a,b_1)$ where $b_1$ is
obtained by taking the first digit of $b$. If we then still have less than
three records, we take records of group $a$ to determine the value for $(a,b)$.

The following tables illustrate the idea. The left table represents the data
set to be aggregated by $A$ and $B$.  The table on the right represents the
output. Colors indicate which data was used. 
%
\begin{center}
\begin{tabular}{cccl}
\multicolumn{4}{l}{Input data}\\
\hline
$A$ & $B$ & $B1$ & $Y$\\
\hline
\cellcolor{red!25}   1 &\cellcolor{red!25}   11 &                1       & \cellcolor{red!25}   1   \\
\cellcolor{red!25}   1 &\cellcolor{red!25}   11 &                1       & \cellcolor{red!25}   2   \\
\cellcolor{red!25}   1 &\cellcolor{red!25}   11 &                1       & \cellcolor{red!25}   4   \\
\cellcolor{green!25} 2 &                     12 & \cellcolor{green!25} 1 & \cellcolor{green!25} 8   \\
\cellcolor{green!25} 2 &                     12 & \cellcolor{green!25} 1 & \cellcolor{green!25} 16  \\
\cellcolor{green!25} 2 &                     13 & \cellcolor{green!25} 1 & \cellcolor{green!25} 32  \\
\cellcolor{blue!25}  3 &                     21 &                2       & \cellcolor{blue!25}  64  \\
\cellcolor{blue!25}  3 &                     22 &                2       & \cellcolor{blue!25}  128 \\
\cellcolor{blue!25}  3 &                     12 &                1       & \cellcolor{blue!25}  256 \\
\hline
\end{tabular}\hspace{1cm}\begin{tabular}{ccl}
\multicolumn{3}{l}{Output}\\
\hline
$A\times B$ & \code{level} & $t_Y$\\
\hline
\rowcolor{red!25}   1 11        & 0             & 7  \\
\rowcolor{green!25} 2 12        & 1             & 56 \\
\rowcolor{green!25} 2 13        & 1             & 56 \\
\rowcolor{blue!25}  3 21        & 2             & 448\\
\rowcolor{blue!25}  3 22        & 2             & 448\\
\rowcolor{blue!25}  3 12        & 2             & 448\\
\hline
\end{tabular}
\label{eq:example}
\end{center}
%
The First row in the output represents group $(A=1,B=11)$. The collapsing level
is zero, which means that no collapsing was necessary. Indeed, in the data
table we see that there are three rows with $A=1$ and $B==11$ with $Y$ values
$1, 2$, and $7$, resulting in $t_Y=7$ for this group.

Next, we try to compute the total for group $(A=2,B=12)$ (in green) but find
that there are only two such rows. We now define a new group $(A=2,B_1=1)$ and
find that there are three records in that group so we get 
$t_Y=8+16+32=56$.  Similarly, there is only one record with $(A=2,B=13)$. 
Collapsing groups to $(A=2,B1=1)$, yields again $t_y=56$.

Finally, for $(A=2,B=21)$ there is only a single record. Collapsing to
$(A=2,B1=2)$ yields only two records, so we need to collapse further to $(A=2)$
and finally obtain three records.  This yields $t_Y=64+128+256=448$. Similarly,
the groups $(A=3, B=22)$ and $(A=3,B=12)$ are collapsed to $(A=2)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5%%%%%%%
\section{The accumulate R package}\label{sect:accumulate}
Grouped aggregation with a fall-though scenario based on a collapsing scheme
requires a fair amount of specification by the user. Besides the data to be
aggregated, one needs to specify the method(s) of aggregation, the collapsing
scheme, and the condition to decide whether a subset is fit for aggregation or
a next collapse is necessary. There are two main functions in \pkg{accumulate}
that offer slightly different interfaces.
\begin{verbatim}
  accumulate(data, collapse, test, fun, ...)
  cumulate(data, collapse, test, ...)
\end{verbatim}
Here \texttt{data} is a data frame holding data to be aggregated;
\texttt{collapse} represents the collapse sequence (as a \class{formula} or a
\class{data frame}), and \texttt{test} is a function that accepts a subset of
\texttt{data} and returns a boolean that indicates whether a subset is suited
for aggregation or not. In \code{accumulate()}, the parameter \code{fun}
represents an aggregation function that is applied to every column of
\code{data}, and the ellipsis (\code{...}) is for arguments that are passed as
extra argument to \code{fun}. The interface of \code{accumulate()} is modeled
after the \code{aggregate()} function in \proglang{R}.  In \code{cumulate}, the
ellipsis is a sequence of comma-separated \code{name=expression} pairs in the
style of \code{summarise()} from the \code{dplyr} package.

The output of both functions are of the same form. The columns of the output
data frame and can schematically be represented as follows. 
\begin{verbatim}
  [Grouping Variables, Collapse level, Output aggregates]
\end{verbatim}
The first columns represent the variables that define the output grouping, the
next column is an integer that indicates the level of collapsing used to
compute the aggregate (0 indicating no collapse), and the last set of columns
store the aggregates.

Both functions support two different interfaces for specifying collapsing
schemes through the \code{collapse} parameter. The first and most general is
the formula interface, which requires that the collapsing sequence is
represented as variables in the data set to be aggregated. The second is the
tabular interface, where a table similar to~\eqref{eq:tablerep} is expected. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The formula interface}\label{sect:formula}
We will use the example of Section~\ref{sect:dynamicgrouping} to illustrate
the formula interface.
<<>>=
library(accumulate)
input <- data.frame(
     A  = c(1,1,1,2,2,2,3,3,3)
   , B  = c(11,11,11,12,12,13,21,22,12)
   , B1 = c(1,1,1,1,1,1,2,2,1)
   , Y  = 2^(0:8)
)

cumulate(input, collapse = A*B ~ A*B1 + A
        , test = function(d) nrow(d) >= 3, tY = sum(Y) )
@
%
Consider the formula \code{A*B ~ A*B1 + A} in the call to \code{cumulate()}.
The left-hand-side \code{A*B} is the target output grouping. The
right-hand-side is to be interpreted as the collapsing sequence: if an instance
of \code{A*B} does not pass the test, then collapse to \code{A*B1}, and if that
does not pass the test collapse to \code{A}. If this final grouping also does
not pass the test, the result is \code{NA}.

Summarizing, the formula interface is always of the following form.
\begin{verbatim}
  Target grouping ~ Alternative1 + Alternative2 + ... + AlternativeN
\end{verbatim}



It is possible to get the same result with \code{accumulate()}. This will cause
summation over all variables that are not used in the formula object. In the
below example we also introduce the helper function \code{min_records()}.
<<>>=
input$Y2 <- 3^(0:8)
accumulate(input, collapse = A*B ~ A*B1 + A
          , test = min_records(3), fun = sum)
@
This means that for experimentation, users must be careful to exclude categorical
variables that are not used from the input data set. For example, if \code{B1}
is not used, we get the following.
<<>>=
accumulate(input[-3], collapse = A*B ~ A
          , test = min_records(3), fun = sum)
@

The formula interface allows users to quickly experiment with different
collapsing schemes. It does require that all categorical variables have been
added to the data. As an alternative, one can use the data frame specification,
which allows for a further separation between defining the collapsing scheme
and the actual data processing.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The data frame interface} \label{sect:dfint}
The data frame interface is somewhat limited because it only allows for a
single grouping variable. The advantage however setting up a collapsing scheme
in the form of a table closely connects to domain knowledge and allows
fine-grained control on how groups are collapsed.

In order to use the data frame interface, the input dataset must include the
most fine-grained grouping variable. In the running example this is $A\times
B$, so we need to combine that into a single variable, and remove the other
ones.
<<>>=
input1 <- input
input1$AB <- paste(input$A, input$B, sep="-")
input1 <- input1[-(1:3)]
input1
@
We now define the collapsing scheme as follows.
<<>>=
csh <- data.frame(
  AB  = c("1-11", "2-12","2-13","3-21","3-22","3-12")
, AB1 = c("1-1" , "2-1" ,"2-1" ,"3-2" ,"3-2" ,"3-1" )
, A   = c("1"   , "2"   ,"2"   ,"3"   ,"3"   ,"3"   ))
csh
@
In this data frame, two consecutive columns should be read as a child-parent
relation. For example, in the first collapsing step the groups defined by
\code{AB == "2-12"} and \code{AB=="2-13"} both collapse to \code{AB1 == "2-1"}.
In this artificial example the codes do not mean anything, but in realistic
cases where codes represent a (hierarchical) classification, domain experts
usually have a good grasp of which codes can be combined. 

The calls to \code{cumulate()} and \code{accumulate()} now look as follows.
<<>>=
accumulate(input1, collapse=csh, test = min_records(3), sum)
cumulate(input1, collapse=csh, test=min_records(3), tY=sum(Y), tY2 = sum(Y2))
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Balanced and unbalanced hierarchical classifications}
Hierarchical classifications are abundant in (official) statistics.
They represent a classification of entities into non-overlapping, nested
groupings. Examples include the International Standard Industrial Classification
of Economic Activities (ISIC, \citet{un2022isic}), the related 
Statistical Classification of Economic Activities in Europe (NACE, \citet{eu2006nace})
and the Euroean Skills, Competences, Qualifications and Occupations classification
(ESCO, \citet{eu2022esco}).

Hierarchical classifications offer a natural mechanism for collapsing
fine-grained groupings into larger groups because of the parent-child
relationships. As an example consider a small piece of the NACE classification.

\begin{center}
\begin{tikzpicture}
\tikzstyle{level 1}=[level distance=10mm,sibling distance=40mm]
\tikzstyle{level 2}=[level distance=10mm,sibling distance=10mm]
  \node {\code{01}}
    child { node {\code{011}}
      child {node {\code{0111}}}
      child {node {\code{0112}}} 
      child {node {\code{0113}}} }
    child { node {\code{012}}
      child {node {\code{0121}}}
      child {node {\code{0122}}} 
      child {node {\code{0123}}} 
      child {node {\code{0124}}} };
\end{tikzpicture}
\end{center}
Here, the hierarchy suggest to collapsing $\{0111,0112,0113\}$ into $\{011\}$
(when needed) and similar for the right branch. The second level of collapsing
would combine $\{012\}$ with $\{011\}$ into $\{01\}$. The \pkg{accumulate}
package comes with a helper function that creates the collapsing scheme from the
lowest-level digits.
<<>>=
nace <- c("0111","0112","0113","0121","0121","0122","0123","0124")
csh_from_digits(nace, levels=2)
@ 
Here, the parameter \code{levels} determines how many collapsing steps will be
computed. Since all codes are prepended with zero, there is no need to collapse
01 any further. The output can be used as argument to the \code{collapse}
parameter of the \code{accumulate()} or \code{cumulate} functions.

The situation becomes a little more involved when hierarchical classifications
form a tree such that the distance from leave to trunk is not the same for all
leaves (unbalanced tree). This occurs in practice, for example when local
organisations create an extra level of detail for some, but not all leaves.
Below is an example of such a situation.
%
\begin{center}
\begin{tikzpicture}
\tikzstyle{level 1}=[level distance=10mm,sibling distance=50mm]
\tikzstyle{level 2}=[level distance=10mm,sibling distance=10mm]
\tikzstyle{level 2}=[level distance=10mm,sibling distance=15mm]
  \node {\code{01}}
    child { node {\code{011}}
      child {node {\code{0111}}}
      child {node {\code{0112}}} 
      child {node {\code{0113}}} }
    child { node {\code{012}}
      child {node {\code{0121}}}
      child {node {\code{0122}}} 
      child {node {\code{0123}}} 
      child {node {\code{0124}}
       child {node {\code{01241}}}
       child {node {\code{01242}}} } };
\end{tikzpicture}
\end{center}
%
In this case, not all leaves can be collapsed with the same number of
collapsing levels. This provides an issue for specifying the collapsing
sequence as it now depends on the leaf where you start how many collapsing
levels are possible. It also complicates interpretability of the result as the
collapsing level reported in the output, now means different things for
different target groups. The solution chosen in \pkg{accumulate} is to extend
the tree by making copies of the leaves that are not on the lowest level as
follows.
%
\begin{center}
\begin{tikzpicture}
\tikzstyle{level 1}=[level distance=10mm,sibling distance=70mm]
\tikzstyle{level 2}=[level distance=10mm,sibling distance=20mm]
\tikzstyle{level 3}=[level distance=10mm,sibling distance=15mm]
  \node {\code{01}}
    child { node {\code{011}}
      child {node {\code{0111}}
       child {node{\code{0111}}}} 
      child {node {\code{0112}} 
       child {node{\code{0112}}}} 
      child {node {\code{0113}}
       child {node{\code{0113}}}} } 
    child { node {\code{012}}
      child {node {\code{0121}}
       child {node {\code{0121}}}}
      child {node {\code{0122}}
       child {node{\code{0122}}}} 
      child {node {\code{0123}}
       child {node{\code{0123}}}} 
      child {node {\code{0124}}
       child {node {\code{01241}}}
       child {node {\code{01242}}} } };
\end{tikzpicture}
\end{center}
%
The tradeoff is that although there may be some extra calculations in the case
where a leaf is collapsed to itself. The gain is that the specification of the
calculation as well as the interpretation of the results are now uniform
across all hierarchical classifications. Again, using \code{csh_from_digits()}
deriving the collapsing scheme can be automated.
<<>>=
nace <- c("0111","0112","0113","0121","0122","0123","01241","01242")
csh_from_digits(nace,levels=3)
@







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Specifying tests}
The \code{test} parameter of \code{cumulate()} and \code{accumulate} accepts a
function that takes a subset of the data and must returns a boolean. For common
test conditions, including requiring a minimal number of records, or a minimal
number or fraction of complete records there are helper functions available.
%
\begin{center}
\begin{tabular}{ll}
\code{min_records(n)}        & At least $n$ records.\\ 
\code{min_complete(n,vars)}  & At least $n$ records complete records for variables \code{vars}.\\
\code{frac_complete(r,vars)} & At least $100r$\% complete records for variables \code{vars}.\\
\end{tabular}
\end{center}

Second, for multiple, possibly complex requirements on variables users can
express conditions with the \pkg{validate} package \cite{loo2021data}.  The
\pkg{validate} packages offers a domain-specific language for expressing,
manipulating, and investigating conditions on datasets. It's core concept is a
list of `data validation rules' stored as a \class{validator} object. A
\class{validator} object is constructed with the eponymous function
\code{validator()}.  For example, to demand that there are at least 3 rows in a
group, and that there are at least three records where $Y\geq 2$ we create the
following ruleset.
<<>>=
library(validate)
rules <- validator(nrow(.) >= 3, sum(Y >= 2) >= 3)
rules
@
Here the \code{.} refers to the dataset as a whole, while rules that can be
evaluated within the dataset can be written as boolean \proglang{R}
expressions.

We will apply these conditions to the \code{input} dataset that was 
constructed in Section~\ref{sect:formula}. As a reminder we print
the first few records.
<<>>=
head(input, 4)
@
We use \code{A*B ~ A*B1 + B1} as collapsing scheme. The function
\code{from_validator} passes the requirements as a test function to
\code{accumulate()} (or \code{cumulate()}).
<<>>=
accumulate(input, collapse = A*B ~ A*B1 + B1,
    test = from_validator(rules), fun=sum)
@
Note that for target groups $(A=2,B=21)$ and $(A=2,B=22)$ none of the available
collapsing levels lead to a group that satisfied all conditions. Therefore the
collapsing level and output variables are all missing (\code{NA}).

The third and most flexible way for users to express tests is to write a custom
testing function. The requirements are that it must work on any subset of a
data frame, including a dataset with zero rows. The previous example can
thus also be expressed as follows.
<<>>=
my_test <- function(d) nrow(d) >= 3 && sum(d$Y >= 2) >=3
accumulate(input, collapse = A*B ~ A*B1 + B1, test=my_test, fun=sum)
@

It is easy to overlook some edge cases when specifying test functions.
Recall that a test function is required to return \code{TRUE} or \code{FALSE},
regardless of the data circumstances. The only thing that a test function
can assume is that the received data set is a subset of records from the
dataset to be aggregated. As a service to the user, \pkg{accumulate} exports
a function that checks a test function against common edge cases, including
the occurrence of missing values, a dataset with zero rows, and the full
dataset. The smoke test checks whether the output is \code{TRUE} or \code{FALSE}
under all circumstances and also reports errors, warnings, and messages.
It accepts a (realistic) dataset and a testing function and prints
test results to the console.
<<>>=
smoke_test(input, my_test)
@
By default only failing tests are printed. In this case our test function
is not robust against missing values for $Y$. This can be remedied
by adding \code{na.rm=TRUE} to the \code{sum()} in the test function.
<<>>=
my_test1 <- function(d) nrow(d) >= 3 && sum(d$Y>=2, na.rm=TRUE) >=3 
smoke_test(input, my_test1)
@
The smoke test is aimed at preventing complicated stack traces when errors occur
in a call to \code{accumulate()} or \code{cumulate()}.  Users should be aware
that it does not guarantee correctness of the results, only robustness against
certain edge cases.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extensive example: economic data} \label{sect:example}
In this section we treat three practical examples using a synthetic
dataset included with the package.
<<>>=
data(producers)
head(producers)
@
This \code{producers} dataset contains synthetic data records of various income
sources for \Sexpr{nrow(producers)} industrial produces. The records are
classified into a local version of the NACE classification called \code{sbi}
and into \code{size} classes with values in $\{5,6,7,8,9\}$. 

\subsection{Small area estimation} 
Small area estimation (SAE) is a collection of methods that are aimed at
estimating subpopulation parameters in cases where there the number of
observations in a subpopulation is so mall that direct estimation leads to
unacceptable estimation variance. Instead one resorts to indirect estimation,
meaning that one estimates the parameter for a larger subpopulation that
includes the target subpopulation. This estimate is then interpreted as an
estimate for the target subpopulation. Here, we shall be interested in
estimating the average turnover from industrial activities (\code{industrial})
by SBI and size class.

In the simplest case, where no auxiliary information is available or used, one
replaces the estimator of the mean over a subpopulation with the estimator of
the mean over a larger subpopulation. If we assume that all units in the
dataset were sampled with equal probability based on a simple random sampling
design, the mean is estimated with the sample mean. We will demand that there
are at least ten records for which turnover has been measured. The collapsing
scheme is given by \code{sbi*size ~ sbi + sbi2 + sbi1} where \code{sbi2} and
\code{sbi1} are classification by respectively the first two SBI digits and the
first SBI digit. We first add those variables to the dataset.
<<>>=
producers$sbi2 <- substr(producers$sbi,1,2)
producers$sbi1 <- substr(producers$sbi,1,1)
head(producers, 3)
@
Our test function looks as follows.
<<>>=
testfun <- function(d) sum(!is.na(d$industrial)) >= 10
@
Using \code{accumulate()} we obtain the means. 
<<>>=
a <- cumulate(producers
             , collapse = sbi*size ~ sbi + sbi2 + sbi1
             , test = testfun
             , mean_industrial = mean(industrial, na.rm=TRUE))
head(a,3)
@

\subsection{Imputing missing values using SAE and ratio imputation} 
Our goal in this example is to impute missing values for the \code{industrial}
variable based on ratio imputation with \code{total} as predictor.  Ratio
imputation is a method where the imputed value $\hat{y}_i$ for variable $Y$ of
record $i$ is estimated as $\hat{y_i}=\hat{R}_d x_i$, where $\hat{R}$ is an
estimate of the ratio between $Y$ and an auxiliary variable $X$ in
subpopulation $d$. An unbiased estimate for $R_d$ is given by
$\hat{\bar{Y}}_d/\hat{\bar{X}}_d$ where $\hat{\bar{Y}}_d$ and $\hat{\bar{X}}_d$
are estimated subpopulation means. 

We use SAE to estimate the subpopulation ratios, and then use the \code{simputation}
package \citep{loo2022simputation} to impute the missing values. 
<<>>=
r <- cumulate(producers
         , collapse = sbi*size ~ sbi + sbi2 + sbi1
         , test = testfun
         , R = mean(industrial, na.rm=TRUE)/mean(total, na.rm=TRUE))
head(r,3)
@
To impute the values, using \code{impute_proxy()} we need to merge the ratios
with the producers dataset (which automatically happens by SBI and size class).
<<>>=
library(simputation)
dat <- merge(producers, r)
dat <- impute_proxy(dat, industrial ~ R*total)
@
We can inspect the imputed values as follows.
<<>>=
iNA <- is.na(producers$industrial)
head(dat[iNA, c("sbi","size","level","R","industrial","total")])
@
From this, we have all the information to interpret the imputed values. For
example, we see that in record 664, the ratio was estimated after collapsing
the group $(\texttt{sbi},\texttt{size})=(21122,7)$ to $\texttt{sbi2}=21$ since
the collapse \code{level} equals 2.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Formal description and algorithms}
In this section we give a formal description of the algorithm for aggregation
with dynamic grouping.  We start by giving an algorithm for ordinary
split-apply-combine to show where the algorithm must be generalized to allow
for a collapsing scheme.

\subsection{Split-apply-combine}
To analyse a data set group by group we need to specify a data set, a
way to split it into groups, and a function that takes a subset of data and
returns an aggregate. Let us introduce some notation for that.

Denote with $U$ be a finite set, and and let $\phi:2^U\to X$ be a function that
accepts a subset of $U$ and returns a value in some domain $X$.  Here, $U$
represents a data set, $2^U$ its power set, and $\phi$ an aggregating function.
We split $U$ into groups using the following notation. Let $A$ be finite set
that has no more elements than $U$, and let $f:\onto A$ be a
surjective function that takes an element of $U$ and returns a value in $A$. We
can think of $A$ as a set of group labels, and $f$ as the function that assigns
a label to each element of $U$. This way, $f$ divides $U$ into non-overlapping
subsets. We say that $f:U\onto A$ is a \emph{partition} of $U$. We
also introduce the \emph{pullback along $f$}, $f^*:2^A\to 2^U$ defined as
%
\begin{equation*}
f^*(S) = \{u\in U|f(u)\in S\},
\end{equation*}
where $S$ is a subset of $A$ \citep[Section 1.4]{fong2019invitation}.

In this notation, any split-apply-combine operation can be computed with the
following algorithm.

\begin{algorithm}[H]
\caption{Split-Apply-Combine}
\label{alg:sac}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{A finite set $U$, an aggregator $\phi: 2^U\to X$, and a partition
       $f:U\onto A$.}
\Output{$R$: the value of $\phi$ for every part of $U$ as a set of pairs
        $(a,x)\in A\times X$. }
$R = \{\}$\;
\For{$a\in A$}{
  $d=f^*(\{a\})$\tcp*{get subset of $U$}
  $R = R\cup \{(a,\phi(d))\}$\tcp*{aggregate and add to result}
}
\end{algorithm}
%
In this algorithm the output is collected in a set $R$ containing pairs from
$A\times X$: one pair for each element of $A$. Incidentely, the algorithm can
be summarized even shorter in this notation as $R=\cup_{a\in A}\{(a,(\phi\circ
f^*)(\{a\}))\}$, where $\circ$ denotes function composition.  Since $f^*$
always receives a singleton set, its definition as a function of the powerset
of the group labels $A$ is too general for this algorithm.  However, this
generalization turns out to be necessary when generalizing the algorithm to the
case of dynamic grouping.

It is interesting to see how the elements $U$, $f:U\onto A$, and
$\phi$ are implemented in practice. Consider the signature of the
\proglang{R}'s \code{aggregate()} function (we skip arguments that are not
important for the discussion).
\begin{verbatim}
  aggregate(x, by, FUN)
\end{verbatim}
Here, \code{x} is a data frame where each column is a variable $U$ to be
aggregated.  The parameter \code{by} is a list of vectors of group labels,
where each vector has a with a length that equals the number of rows in
\code{x}. So the function $f:U\onto A$ is implemented by asking
the user to make sure that the position of each label corresponds to the
correct row number in the data frame \code{x}. The argument \code{FUN} is the
function $\phi$ that aggregates each subset of \code{x}.  Here is an example of
how a user might call this function from the R prompt.
%
<<>>=
aggregate(iris[1:2], by=iris["Species"], FUN=mean)
@
%
Note that the correspondence in position of the \texttt{Species} label and the
record position is implemented by taking them from the same data frame. The
output also reveals in the first column the set $A$: each row corresponds to a
unique value in \texttt{Species} column.


\subsection{Split-apply-combine with collapsing groups}
\label{sect:saccg}
The goal of the algorithm is to compute a value for each part of a dataset,
possibly using values external to the part. The input of the algorithm consists
again of a finite set $U$ and an aggregation function $\phi$ that takes a
subset of $U$ and returns a value in some domain $X$.  Compared to
Algorithm~\ref{alg:sac} two different inputs are needed. First, a function must
be defined that that checks whether a given subset $d$ of $U$ is suitable for
computing $\phi(d)$.  We will call this function $\beta: 2^U\to \mathbb{B}$,
where $\mathbb{B}=\{\texttt{True},\texttt{False}\}$.  Typical tests are
checking whether there are sufficient records available, or whether certain
variables have a low enough fraction of missing values. Second, we need a
\emph{collapsing scheme}
\begin{equation}
U\xonto{f}A\xonto{f_1}A_1\xonto{f_2}\cdots\xonto{f_n}A_n.
\label{eq:collapsingsequence}
\end{equation}
A collapsing scheme is a sequence of partitions where each $f_i$ partitions its
domain in $|A_i|$ groups while $f$ partitions $U$ in $|A|$ groups.


Denote with $F_k:A\to A_k$, the function that  accepts a label in $A$ and
returns the corresponding label in $A_k$. In other words, $F_k$ is the
composition $f_k\circ f_{k-1}\circ\cdots \circ f_1$. Similarly we define the
pullback along $F_k$ as $F_k^* = f_1^*\circ f_2^*\circ\cdots\circ f_k^*$.  This
function accepts a set of labels in $A_k$ and returns all the labels in $A$
that are mapped to those labels via the collapsing sequence of
Equation~\eqref{eq:collapsingsequence}.  With this notation we can define the
Algorithm for aggregation with dynamic grouping as follows. 

\begin{algorithm}[H]
\caption{Split-Apply-Combine with Collapsing Groups}
\label{alg:saccg}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{A finite set $U$, an aggregator $\phi: 2^U\to X$, a test function $\beta: 2^U\to \mathbb{B}$,
      and a collapsing sequence $U\xonto{f}A\xonto{f_1}A_1\xonto{f_2}\cdots \xonto{f_n}  A_n$.}

\Output{$R$: the value of $\phi$ for every part of $U$, for which a suitable
collapsing group can be found, as a set of triples $(a,k,x)\in A\times
\underline{n}\times X $ where $\underline{n}=\{0,1,\ldots,n\}$.}

$R = \{\}$\;
\For{$a\in A$}{
  $i=0$ \tcp*{Initiate collapse level}
  $d = f^*(\{a\})$ \tcp*{Get subset of $U$}
  \While{$i<n \land \lnot\beta(d)$}{ \label{line:while}
    $i = i+1$ \tcp*{Increase collapse level} 
    $d = (f^*\circ F_i^*\circ F_i)(a)$ \tcp*{Collapse and get subset}
  }\label{line:endwhile}
  \If{$i<n \lor \beta(d)$}{ \label{line:cond}
    $R = R\cup \{(a, i,\phi(d))\}$\; \label{line:R}
  } 
}

\end{algorithm}
%

In this algorithm the collapsing level $i$ is increased until the test is
passed or the maximum collapsing level $n$ is reached
(Lines~\ref{line:while}-\ref{line:endwhile}) %Since the eventual level of
collapsing is determined dynamically by data %circumstances in $U$, the
algorithm also reports the collapsing level $i$ used.  The condition in
Line~\ref{line:cond} ensures that if no suitable dataset is found after the
whole collapsing sequence has been executed, then no answer is returned. This
means that in contrast with Algorithm~\ref{alg:sac} there is no guarantee that
a value for each member of $A$ will be found. For each member of $A$ where
an aggregate is computed, there is a triple $(a,i,\phi(d))$, where $a\in A$
is the label to which the value $\phi(d)$ pertains, and $i$ is the number of
collapses applied to reach a suitable dataset. 


Comparing Algorithms~\ref{alg:sac} and~\ref{alg:saccg}, we see that the
standard split-apply-combine algorithm has worst-case runtime complexity
$O(|A|)$, which is equal to the best case $\Omega(|A|)$. The
split-apply-combine with collapsing groups algorithm has best case
$\Omega(|A|)$ equal to standard split-apply-combine but worst case $O(n|A|)$.
In fact, the best case for Algorithm~\ref{alg:saccg} is achieved by setting
$\beta:d\mapsto\texttt{True}$. It is not hard to see that in this case
Algorithm~\ref{alg:saccg} reduces to Algorithm~\ref{alg:sac}. The worst case is
achieved by setting $\beta: d\mapsto \texttt{False}$. In that case the while
loop in Line~\ref{line:while} is iterated $n-1$ times (yielding total $n$
executions of $f^*$) while Line~\ref{line:R} is never executed.







\section{Summary and conclusion}\label{sect:conclusion}

\begin{itemize}
\item{Record-wise results}
\item{Record-wise imputation}
\item{Representation of hierarchical classifications: parent-child lists vs tabular}
\end{itemize}

\newpage
\bibliography{refs}




\end{document}
